{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c5aa590",
   "metadata": {},
   "source": [
    "# Simple grokking demo with superposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d6818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import einops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df155eba",
   "metadata": {},
   "source": [
    "## Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baf055e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "p = 97\n",
    "train_frac  = 0.3\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e6bf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[71, 77, 97],\n",
      "        [91,  9, 97],\n",
      "        [33, 73, 97],\n",
      "        ...,\n",
      "        [ 6, 51, 97],\n",
      "        [13, 39, 97],\n",
      "        [75, 27, 97]])\n",
      "tensor([51,  3,  9,  ..., 57, 52,  5])\n"
     ]
    }
   ],
   "source": [
    "a_vec = einops.repeat(t.arange(p),\"i -> (i j)\",j=p)\n",
    "b_vec = einops.repeat(t.arange(p),\"j -> (i j)\",i=p)\n",
    "eq_vec = einops.repeat(t.tensor(p),\" -> i\",i= p**2)\n",
    "# The dataset consists of pairs (x,y) with x = (a,b,eq) and y = a+b mod p\n",
    "# we randomly permute the dataset and split it into train and test dataset\n",
    "dataset = t.stack([a_vec,b_vec,eq_vec],dim=1).to(device=device)\n",
    "labels = (dataset[:,0] + dataset[:,1]) % p\n",
    "indices = t.randperm(p**2)\n",
    "train_indices = indices[:int(train_frac*p**2)]\n",
    "test_indices = indices[int(train_frac*p**2):]\n",
    "\n",
    "train_dataset = dataset[train_indices]\n",
    "train_labels = labels[train_indices]\n",
    "\n",
    "test_dataset = dataset[test_indices]\n",
    "test_labels = labels[test_indices]\n",
    "print(train_dataset)\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2331a5f1",
   "metadata": {},
   "source": [
    "## Create a simple one layer transformer model\n",
    "\n",
    "This is sort of the easiest possible transformer model. It takes as input the one hot encoded token sequence (a,b,=), where = is set to p for convenience. The architecture is:\n",
    "\n",
    "0. Token: the tokens $t_0,t_1,t_2$ are one hot encoded d_vocab dimensional vectors and the input sequence is $$t = (t_0,t_1,t_2)^T$$\n",
    "1. Embedding: The tokens are embedded in the d_model dimensional space by a learnable matrix W_E $$x_0 = Embed(t) = t @W_E$$\n",
    "2. Positional Embedding: The positional Embedding is implemented by adding a d_model dimensional vector to each embedded vector, depending on the token position. It is also learned. $$x_1 = x_0 + W_{pos}$$\n",
    "3. Attention layer: A simple attention layer with num_heads = 4 attention heads\n",
    "$$x_2 = x_1 + Attention(x_1)$$\n",
    "4. MLP layer: A simple mlp with one hidden layer with ReLU activation function and no bias:\n",
    "$$x_3 = x_2 + MLP(x_2)$$\n",
    "5. Unembedding: a learned unembedding matrix W_U that maps back to the vocab.\n",
    "$$x_4 = x_3 @ W_U$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5463f453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config for the transformer architecture\n",
    "d_vocab = p+1   #The input consists of the numbers from 0 to p-1 and p for the equal sign\n",
    "d_model = 128   #dimension of the model\n",
    "n_ctx = 3       #context length (a,b,=) where = is encoded as 97\n",
    "num_heads = 4\n",
    "d_heads = d_model//num_heads\n",
    "d_mlp = 4 * d_model\n",
    "act_type = \"ReLU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60cf6e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.one_layer_transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fb75c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(d_model,d_mlp,d_heads,d_vocab,num_heads,n_ctx,act_type).to(device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e6bad",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96a7b29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ncl/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import tqdm.auto as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdf33d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training parameters\n",
    "n_epoch = 30000\n",
    "lr = 1e-3\n",
    "wd = 1.\n",
    "DATA_SEED = 346\n",
    "betas = (0.9,0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca09317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = t.optim.AdamW(model.parameters(),lr=lr, betas=betas,weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d621d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define cross entropy loss\n",
    "def loss_fn(logits,labels):\n",
    "    if len(logits.shape) == 3:\n",
    "        logits = logits[:,-1]\n",
    "    logits = logits.to(t.float64)\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    correct_log_probs = log_probs.gather(dim=-1,index = labels[:,None])[:,0]\n",
    "    return -correct_log_probs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c1bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 102/30000 [00:09<45:53, 10.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 Train Loss 1.2547569312723175 Test loss 8.574755202910724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 202/30000 [00:18<43:30, 11.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199 Train Loss 0.013822463495749922 Test loss 14.162979331560935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 302/30000 [00:27<43:19, 11.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 Train Loss 0.004671921987955941 Test loss 14.622254524056357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 402/30000 [00:37<42:56, 11.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 399 Train Loss 0.0015216687051994378 Test loss 15.364470201620046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 502/30000 [00:46<46:49, 10.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 499 Train Loss 0.0005052167135453146 Test loss 16.17842971119118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 600/30000 [00:55<47:02, 10.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 599 Train Loss 0.00017019457442162453 Test loss 17.01893632245188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 701/30000 [01:04<44:31, 10.97it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 699 Train Loss 5.809391899440574e-05 Test loss 17.864302541335043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 800/30000 [01:14<48:44,  9.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 799 Train Loss 2.017687304490978e-05 Test loss 18.683733216788905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 901/30000 [01:24<47:50, 10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 899 Train Loss 7.221343434953348e-06 Test loss 19.46701321674217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1002/30000 [01:34<48:55,  9.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999 Train Loss 2.73839857305549e-06 Test loss 20.17284289283619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 1102/30000 [01:43<40:14, 11.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1099 Train Loss 1.1617990829398628e-06 Test loss 20.753033357796085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1201/30000 [01:53<46:16, 10.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1199 Train Loss 5.91560957787267e-07 Test loss 21.145291375407506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1302/30000 [02:02<48:05,  9.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1299 Train Loss 3.794066796278684e-07 Test loss 21.312957057784033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1402/30000 [02:11<45:07, 10.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1399 Train Loss 3.0340388641008003e-07 Test loss 21.25736111161941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1501/30000 [02:20<41:50, 11.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1499 Train Loss 2.808090669862008e-07 Test loss 21.053494335543952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1601/30000 [02:29<40:00, 11.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1599 Train Loss 2.7546344729555363e-07 Test loss 20.796509646680743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1701/30000 [02:38<42:17, 11.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1699 Train Loss 2.7377221906698215e-07 Test loss 20.51771328110459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1801/30000 [02:47<42:24, 11.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1799 Train Loss 2.726842892929827e-07 Test loss 20.227458235382713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 1901/30000 [02:56<46:10, 10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1899 Train Loss 2.714062681167369e-07 Test loss 19.929136759903816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2002/30000 [03:06<43:50, 10.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1999 Train Loss 2.6985457253635636e-07 Test loss 19.616734280234798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2030/30000 [03:08<39:49, 11.71it/s]"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in tqdm.tqdm(range(n_epoch)):\n",
    "    train_logits = model(train_dataset)\n",
    "    train_loss = loss_fn(train_logits,train_labels)\n",
    "    train_loss.backward()\n",
    "    train_losses.append(train_loss.item())\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    with t.inference_mode():\n",
    "        test_logits = model(test_dataset)\n",
    "        test_loss = loss_fn(test_logits,test_labels)\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "    if ((epoch+1)%100)==0:\n",
    "        print(f\"Epoch {epoch} Train Loss {train_loss.item()} Test loss {test_loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caeb03f",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84d9f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f5c884",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses,label=\"Training Loss\")\n",
    "plt.plot(test_losses,label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Log Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Traing and Test loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
